{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In the past notebook, PCA was applied to see if there was a better combination of parameters that could improve the predictions. PCA was applied to the original inputs (18 of them), and PCA models that explained 99%, 95%, 90%, and 80% were tried out. The predictions had a positive **mean** but the minimum was still negative. Additionally, the errors were still through the roof. \n",
    "\n",
    "For that notebook MinMax Scaler was replaced by Standard Scaler. In this notebook, both will be used to compare their performance. \n",
    "\n",
    "Here **all** the variables that were found to be making the predictions negative in notebook **1_03** will be removed. Then PCA will be applied to explain 99%, 98%, 95%, and 90% of the output variance.\n",
    "\n",
    "Additionally, two extra models will be trained using the original data without PCA (but with the problematic variables removed. \n",
    "\n",
    "The architecture will be: \n",
    "\n",
    "* **MAPE + Leaky ReLU + 20% dropout + linear output**\n",
    "\n",
    "\n",
    "The variables that pushed the predictions to be negative were:\n",
    "\n",
    "    1) Fuel Code\n",
    "    2) Drive System\n",
    "    3) Peak_pos (Positive acceleration peaks)\n",
    "    4) ETW (Estimated Test Weight)\n",
    "    5) a_pos (Positive acceleration mean)\n",
    "    6) HP (Horse Power)\n",
    "\n",
    "After this notebook has been implemented, another smaller hyper-parameter search can be done to see if predictions can be improved. **IF** something in this notebook improves predictions\n",
    "\n",
    "For the sake of continuity, only **HC** will be predicted in this run. If something promising is found, other pollutants will be attempted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## TO DO's\n",
    "\n",
    "Data Scaling\n",
    "* Import scaled data\n",
    "* Import scalers (MinMax)\n",
    "* Inverse scale data\n",
    "* Impor scalers (Standard)\n",
    "* Scale the data \n",
    "\n",
    "Data Sets\n",
    "* Create the data sets the same way as before (without shuffling to keep the same order)\n",
    "\n",
    "PCA\n",
    "* Apply different instances of PCA keeping a different amount of variables.\n",
    "    * The PCA function receives one input: n_componentes\n",
    "        * If **0 < n_components < 1**, this number represents the minimum amount of variance that needs to be explained by the selected components\n",
    "        * If **1 $\\leq$ n_components $\\leq$ number of input parameters**, this number represents the number of components to be kept\n",
    "* Explain the following amount of variance:\n",
    "    * 99%\n",
    "    * 98%\n",
    "    * 95%\n",
    "    * 90%\n",
    "* Create a new training set according to the PCA instances\n",
    "\n",
    "Machine Learning\n",
    "* Create models for each training set\n",
    "* Train\n",
    "* Predict\n",
    "* Calculate error\n",
    "* Rank models\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drllc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Dense, Dropout, advanced_activations, BatchNormalization, LeakyReLU\n",
    "from keras import losses, optimizers, activations\n",
    "import keras.backend as K\n",
    "\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join('.','output')\n",
    "minmax_scaler_path = os.path.join('.','Scalers','MinMax')\n",
    "standard_scaler_path = os.path.join('.','Scalers','Standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Original Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "complete_data_scaled_shuffled = pd.read_csv('Dataset_Scaled_Shuffled.csv')\n",
    "print('Shuffled dataset loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MinMax Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to put all the scalers\n",
    "minmax_scalers = []\n",
    "\n",
    "for i in range(len(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    scaler_filename = os.path.join(minmax_scaler_path,'scaler{}.save'.format(i))\n",
    "    minmax_scaler = joblib.load(scaler_filename)\n",
    "    \n",
    "    minmax_scalers.append(minmax_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Standard Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to put all the loaded scalers\n",
    "standard_scalers = []\n",
    "\n",
    "for i in range(len(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    scaler_filename = os.path.join(standard_scaler_path,'scaler{}.save'.format(i))\n",
    "    standard_scaler = joblib.load(scaler_filename)\n",
    "    \n",
    "    standard_scalers.append(standard_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Scale Data\n",
    "\n",
    "Using the original MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with feature: Year\n",
      "Success with feature: Vehicle_Code\n",
      "Success with feature: Manufacturer_Code\n",
      "Success with feature: Displacement\n",
      "Success with feature: Fuel_System\n",
      "Success with feature: Gears\n",
      "Success with feature: Transmission_Code\n",
      "Success with feature: ETW\n",
      "Success with feature: HP\n",
      "Success with feature: Drive_System_Code\n",
      "Success with feature: Fuel_Code\n",
      "Success with feature: V_avg\n",
      "Success with feature: V_max\n",
      "Success with feature: V_std\n",
      "Success with feature: a_pos\n",
      "Success with feature: a_neg\n",
      "Success with feature: Peak_pos\n",
      "Success with feature: Peak_neg\n",
      "Success with feature: HC\n",
      "Success with feature: CO\n",
      "Success with feature: CO2\n",
      "Success with feature: Nox\n"
     ]
    }
   ],
   "source": [
    "# First, inverse transform all original values from the test_set\n",
    "original_data_inverse = complete_data_scaled_shuffled.copy()\n",
    "\n",
    "for i in range(len(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    col_name = complete_data_scaled_shuffled.columns[i]\n",
    "    \n",
    "    values = original_data_inverse[col_name].values\n",
    "    values = values.astype('float64')\n",
    "    values = values.reshape(values.shape[0],1)\n",
    "    \n",
    "    original_data_inverse[col_name] = minmax_scalers[i].inverse_transform(values)\n",
    "    \n",
    "    print('Success with feature: {}'.format(col_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data\n",
    "\n",
    "Function to scale data according to an input that decides if it is a MinMax or a Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(scaler_type):\n",
    "    \n",
    "    # Select the scalers\n",
    "    if scaler_type == 'MinMax':\n",
    "        \n",
    "        scalers = minmax_scalers\n",
    "        print('Using {} Scalers'.format(scaler_type))\n",
    "        \n",
    "    if scaler_type == 'Standard':\n",
    "        \n",
    "        scalers = standard_scalers\n",
    "        print('Using {} Scalers'.format(scaler_type))\n",
    "        \n",
    "    # Scale the data\n",
    "    \n",
    "    # Copy the data set to avoid altering the original\n",
    "    new_data_scaled = original_data_inverse.copy()\n",
    "    \n",
    "    # DROP THE VARIABLES WE DON'T WANT\n",
    "    new_data_scaled.drop(columns=['Fuel_Code', 'Drive_System_Code', 'Peak_pos', 'Peak_neg', 'ETW', 'a_pos', 'a_neg', 'HP'], \n",
    "                     inplace=True)\n",
    "    \n",
    "    # Loop over the standard_scalers and perform the scaling operation on each column\n",
    "    for i in range(len(new_data_scaled.columns)):\n",
    "\n",
    "        col_name = new_data_scaled.columns[i]\n",
    "\n",
    "        values = new_data_scaled[col_name].values\n",
    "        values = values.astype('float64')\n",
    "        values = values.reshape(values.shape[0],1)\n",
    "\n",
    "        new_data_scaled[col_name] = scalers[i].fit_transform(values)\n",
    "\n",
    "        print('Success with feature: {}'.format(col_name))\n",
    "    \n",
    "\n",
    "    return new_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_types = ['MinMax', 'Standard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Standard Scalers\n",
      "Success with feature: Year\n",
      "Success with feature: Vehicle_Code\n",
      "Success with feature: Manufacturer_Code\n",
      "Success with feature: Displacement\n",
      "Success with feature: Fuel_System\n",
      "Success with feature: Gears\n",
      "Success with feature: Transmission_Code\n",
      "Success with feature: V_avg\n",
      "Success with feature: V_max\n",
      "Success with feature: V_std\n",
      "Success with feature: HC\n",
      "Success with feature: CO\n",
      "Success with feature: CO2\n",
      "Success with feature: Nox\n"
     ]
    }
   ],
   "source": [
    "data_standard_scaled = scale_data('Standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MinMax Scalers\n",
      "Success with feature: Year\n",
      "Success with feature: Vehicle_Code\n",
      "Success with feature: Manufacturer_Code\n",
      "Success with feature: Displacement\n",
      "Success with feature: Fuel_System\n",
      "Success with feature: Gears\n",
      "Success with feature: Transmission_Code\n",
      "Success with feature: V_avg\n",
      "Success with feature: V_max\n",
      "Success with feature: V_std\n",
      "Success with feature: HC\n",
      "Success with feature: CO\n",
      "Success with feature: CO2\n",
      "Success with feature: Nox\n"
     ]
    }
   ],
   "source": [
    "data_minmax_scaled = scale_data('MinMax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "Function to prepare data based on the scaler that was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size = 62511\n",
      "Dev Size = 7814\n",
      "Test Size = 7814\n",
      "Remainder = 0\n"
     ]
    }
   ],
   "source": [
    "# Get number of data points\n",
    "data_points = complete_data_scaled_shuffled.shape[0]\n",
    "\n",
    "# Set sizes for train, dev, test sets\n",
    "train_percent = 0.8\n",
    "train_size = round(train_percent*data_points)\n",
    "\n",
    "if (data_points-train_size)%2 == 0:\n",
    "    dev_size = int((data_points-train_size)/2)\n",
    "    test_size = dev_size\n",
    "    print('Train Size = {}'.format(train_size))\n",
    "    print('Dev Size = {}'.format(dev_size))\n",
    "    print('Test Size = {}'.format(test_size))\n",
    "    print('Remainder = {}'.format(train_size+dev_size+test_size-data_points))\n",
    "    \n",
    "else:\n",
    "    train_size = train_size-1\n",
    "    dev_size = int((data_points-train_size)/2)\n",
    "    test_size = dev_size \n",
    "    print('Train Size = {}'.format(train_size))\n",
    "    print('Dev Size = {}'.format(dev_size))\n",
    "    print('Test Size = {}'.format(test_size))\n",
    "    print('Remainder = {}'.format(train_size+dev_size+test_size-data_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sets(scaler_type):\n",
    "    \n",
    "    if scaler_type == 'MinMax':\n",
    "        \n",
    "        new_data_scaled = data_minmax_scaled\n",
    "        \n",
    "    if scaler_type == 'Standard':\n",
    "        \n",
    "        new_data_scaled = data_standard_scaled\n",
    "    \n",
    "    # Create a local copy of the entire NEW dataset\n",
    "    data_scaled_shuffled = new_data_scaled.copy()\n",
    "\n",
    "    print('Preparing Data-sets')\n",
    "    # Divide data into train, dev, and test sets\n",
    "    train_set = data_scaled_shuffled[ : train_size]\n",
    "    dev_set = data_scaled_shuffled[train_size : train_size+dev_size]\n",
    "    test_set = data_scaled_shuffled[train_size+dev_size : train_size+dev_size+test_size]\n",
    "\n",
    "    # Reset index for all sets\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "    dev_set = dev_set.reset_index(drop=True)\n",
    "    test_set = test_set.reset_index(drop=True)\n",
    "\n",
    "    # Get values\n",
    "    train_set_values = train_set.values\n",
    "    dev_set_values = dev_set.values\n",
    "    test_set_values = test_set.values\n",
    "\n",
    "    # Number of emissions: HC, CO, CO2, NOX\n",
    "    n_out = 4\n",
    "\n",
    "    print('Splitting into inputs and outputs')\n",
    "    # SLICING: [start row:end row , start column:end column]\n",
    "    # Split into inputs and outputs\n",
    "    x_train = train_set_values[:,:-n_out]\n",
    "    x_dev = dev_set_values[:,:-n_out]\n",
    "    x_test = test_set_values[:,:-n_out]\n",
    "\n",
    "    # Get the outputs (only HC)\n",
    "    HC_train = train_set_values[:,-n_out]\n",
    "    HC_dev = dev_set_values[:,-n_out]\n",
    "    HC_test = test_set_values[:,-n_out]\n",
    "\n",
    "    print('Data-sets complete')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    return x_train, x_dev, x_test, HC_train, HC_dev, HC_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with feature: Year\n",
      "Success with feature: Vehicle_Code\n",
      "Success with feature: Manufacturer_Code\n",
      "Success with feature: Displacement\n",
      "Success with feature: Fuel_System\n",
      "Success with feature: Gears\n",
      "Success with feature: Transmission_Code\n",
      "Success with feature: ETW\n",
      "Success with feature: HP\n",
      "Success with feature: Drive_System_Code\n",
      "Success with feature: Fuel_Code\n",
      "Success with feature: V_avg\n",
      "Success with feature: V_max\n",
      "Success with feature: V_std\n",
      "Success with feature: a_pos\n",
      "Success with feature: a_neg\n",
      "Success with feature: Peak_pos\n",
      "Success with feature: Peak_neg\n",
      "Success with feature: HC\n",
      "Success with feature: CO\n",
      "Success with feature: CO2\n",
      "Success with feature: Nox\n"
     ]
    }
   ],
   "source": [
    "# Inverse transform the TEST DATA to be able to calculate the error further down\n",
    "test_set_scaled = complete_data_scaled_shuffled[train_size+dev_size : train_size+dev_size+test_size]\n",
    "test_set_inverse = test_set_scaled.copy()\n",
    "\n",
    "for i in range(np.size(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    col_name = complete_data_scaled_shuffled.columns[i]\n",
    "    \n",
    "    values = test_set_inverse[col_name].values\n",
    "    values = values.astype('float64')\n",
    "    values = values.reshape(values.shape[0],1)\n",
    "    \n",
    "    test_set_inverse[col_name] = minmax_scalers[i].inverse_transform(values)\n",
    "    \n",
    "    print('Success with feature: {}'.format(col_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Create a function that creates a PCA instance and with that PCA instance creates a new x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca_set(variance_amount, x_train, x_dev, x_test):\n",
    "    \n",
    "    if variance_amount < 1:\n",
    "    \n",
    "        print('Create PCA Instance')\n",
    "        pca = PCA(variance_amount)\n",
    "\n",
    "        print('Fit PCA Instance')\n",
    "        pca.fit(x_train)\n",
    "        print('Number of Components = {}'.format(pca.n_components_))\n",
    "\n",
    "        print('Create New Input Training Set')\n",
    "        new_x_train = pca.transform(x_train)\n",
    "\n",
    "        print('Create New Input Dev Set')\n",
    "        new_x_dev = pca.transform(x_dev)\n",
    "\n",
    "        print('Create New Input Test Set')\n",
    "        new_x_test = pca.transform(x_test)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        new_x_train = x_train\n",
    "        new_x_dev = x_dev\n",
    "        new_x_test = x_test\n",
    "\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    return new_x_train, new_x_dev, new_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_options = [1, 0.99,0.98,0.95,0.90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch size, epochs\n",
    "batch_size = 64\n",
    "epochs = 300\n",
    "dd = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with MAPE, Leaky ReLU, Adam\n",
    "def build_model(number, x_train):\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential(name='Model_{}'.format(number))\n",
    "\n",
    "    model.add(Dense(256, input_dim=x_train.shape[1]))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #Compile model\n",
    "    model.compile(loss=losses.mean_absolute_percentage_error, optimizer=optimizers.Adam(), metrics = ['accuracy'])\n",
    "    \n",
    "    print('{} Created'.format(model.name))\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model, x_train, y_train, x_dev, y_dev):\n",
    "    \n",
    "    print('{} - Training'.format(model.name))\n",
    "    print('- Started on {} at {}'.format(str(datetime.datetime.now())[5:-16], str(datetime.datetime.now())[11:-10]))\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(x_dev, y_dev), verbose=0, shuffle=True)\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time() - start_time\n",
    "    print('{} - Training Complete'.format(model.name))\n",
    "    print('- Time: {:.3f} min'.format(end_time/60))\n",
    "    print('- Loss = {:.5f}'.format(history.history['loss'][-1]))\n",
    "    print('- Val Loss = {:.5f}'.format(history.history['val_loss'][-1]))\n",
    "    print('----------------------------------')\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions and Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define MSPE\n",
    "def msp_error(true,pred):\n",
    "    error = 100*np.sum(((true-pred)/true)**2)/np.size(true)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_get_error(model, x_test, scaler_type):\n",
    "    \n",
    "        # Select the scalers\n",
    "    if scaler_type == 'MinMax':\n",
    "        \n",
    "        scalers = minmax_scalers\n",
    "        \n",
    "    if scaler_type == 'Standard':\n",
    "        \n",
    "        scalers = standard_scalers  \n",
    "    \n",
    "    #------------------\n",
    "    print('Predicting with {}'.format(model.name))\n",
    "    scaled_predictions = model.predict(x_test)\n",
    "    \n",
    "    print('Inverse Scaling Operation') \n",
    "     \n",
    "    # Inverse the scaling operation on the predictions\n",
    "    predictions = scalers[-4].inverse_transform(scaled_predictions)\n",
    "    \n",
    "    print('- Prediction Mean = {:.5f}'.format(np.mean(predictions)))\n",
    "    print('- Prediction Min = {:.5f}'.format(np.min(predictions)))\n",
    "    print('- Prediction Max = {:.5f}'.format(np.max(predictions)))\n",
    "\n",
    "    print('Calculating HC Error')\n",
    "    mspe = msp_error(test_set_inverse['HC'].values, predictions)\n",
    "        \n",
    "    print('- HC Error  = {:.2e}'.format(mspe))\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    return mspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Models and Rank with MSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_models():\n",
    "    \n",
    "    count = 1\n",
    "    model_list = []\n",
    "    history_list = []\n",
    "    HC_error_list = []\n",
    "\n",
    "    for variance_percent in pca_options:\n",
    "        \n",
    "        for scaler_type in scaler_types:\n",
    "\n",
    "            # Print model variables\n",
    "            print('Model_{} Variables:'.format(count))\n",
    "            print('- Loss: MAPE')\n",
    "            print('- Activation: Leaky ReLU')\n",
    "            print('- Optimizer: Adam')\n",
    "            print('- Dropout: {}%'.format(dd*100))\n",
    "            print('- Variance Explained: {}'.format(variance_percent))\n",
    "            print('- Scaler: {}'.format(scaler_type))\n",
    "            print('----------------------------------')\n",
    "            \n",
    "            # Create the inputs and outputs\n",
    "            orginal_x_train, orginal_x_dev, orginal_x_test, y_train, y_dev, y_test = create_data_sets(scaler_type)\n",
    "\n",
    "            # Get the INPUT datasets after PCA\n",
    "            x_train, x_dev, x_test = create_pca_set(variance_percent, orginal_x_train, orginal_x_dev, orginal_x_test)\n",
    "\n",
    "            # Create model\n",
    "            model = build_model(count, x_train)\n",
    "\n",
    "            # Train model\n",
    "            history = train_models(model, x_train, y_train, x_dev, y_dev)\n",
    "            history_list.append(history)\n",
    "\n",
    "            # Make predictions and calculate error\n",
    "            error = predict_get_error(model, x_test, scaler_type)\n",
    "\n",
    "            # Add error to error list\n",
    "            HC_error_list.append([model.name, variance_percent, scaler_type, error])\n",
    "\n",
    "            # Announce one model process ended\n",
    "            print('============== MODEL {} PROCESS END =============='.format(count))\n",
    "            print(' ')\n",
    "\n",
    "            # Increase counter by 1\n",
    "            count = count+1\n",
    "\n",
    "            # Add TRAINED model to list\n",
    "            model_list.append(model)\n",
    "\n",
    "    print('Creating DataFrame')                \n",
    "    HC_error = pd.DataFrame(HC_error_list)\n",
    "\n",
    "    print('Changing DataFrame column names')\n",
    "    HC_error.columns = ['Model', 'Variance Explained', 'Scaler', 'MSPE']\n",
    "\n",
    "    print('Ranking Models')\n",
    "    HC_error.sort_values(by=['MSPE'], inplace=True)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    return HC_error, model_list, history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 1\n",
      "- Scaler: MinMax\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "Model_1 Created\n",
      "----------------------------------\n",
      "Model_1 - Training\n",
      "- Started on 05-16 at 00:40\n",
      "Model_1 - Training Complete\n",
      "- Time: 41.422 min\n",
      "- Loss = 719.23825\n",
      "- Val Loss = 165.20448\n",
      "----------------------------------\n",
      "Predicting with Model_1\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = -0.00209\n",
      "- Prediction Min = -0.00496\n",
      "- Prediction Max = 0.07331\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.25e+08\n",
      "----------------------------------\n",
      "============== MODEL 1 PROCESS END ==============\n",
      " \n",
      "Model_2 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 1\n",
      "- Scaler: Standard\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "Model_2 Created\n",
      "----------------------------------\n",
      "Model_2 - Training\n",
      "- Started on 05-16 at 01:21\n",
      "Model_2 - Training Complete\n",
      "- Time: 42.269 min\n",
      "- Loss = 87.52485\n",
      "- Val Loss = 91.11897\n",
      "----------------------------------\n",
      "Predicting with Model_2\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04048\n",
      "- Prediction Min = 0.00323\n",
      "- Prediction Max = 0.05108\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.06e+10\n",
      "----------------------------------\n",
      "============== MODEL 2 PROCESS END ==============\n",
      " \n",
      "Model_3 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.99\n",
      "- Scaler: MinMax\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 8\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_3 Created\n",
      "----------------------------------\n",
      "Model_3 - Training\n",
      "- Started on 05-16 at 02:04\n",
      "Model_3 - Training Complete\n",
      "- Time: 42.700 min\n",
      "- Loss = 686.36996\n",
      "- Val Loss = 107.70475\n",
      "----------------------------------\n",
      "Predicting with Model_3\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.00210\n",
      "- Prediction Min = 0.00171\n",
      "- Prediction Max = 0.01172\n",
      "Calculating HC Error\n",
      "- HC Error  = 2.02e+08\n",
      "----------------------------------\n",
      "============== MODEL 3 PROCESS END ==============\n",
      " \n",
      "Model_4 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.99\n",
      "- Scaler: Standard\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 9\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_4 Created\n",
      "----------------------------------\n",
      "Model_4 - Training\n",
      "- Started on 05-16 at 02:46\n",
      "Model_4 - Training Complete\n",
      "- Time: 42.248 min\n",
      "- Loss = 87.46589\n",
      "- Val Loss = 88.89074\n",
      "----------------------------------\n",
      "Predicting with Model_4\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04042\n",
      "- Prediction Min = 0.00574\n",
      "- Prediction Max = 0.05138\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.08e+10\n",
      "----------------------------------\n",
      "============== MODEL 4 PROCESS END ==============\n",
      " \n",
      "Model_5 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.98\n",
      "- Scaler: MinMax\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 7\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_5 Created\n",
      "----------------------------------\n",
      "Model_5 - Training\n",
      "- Started on 05-16 at 03:29\n",
      "Model_5 - Training Complete\n",
      "- Time: 41.770 min\n",
      "- Loss = 740.23634\n",
      "- Val Loss = 177.49984\n",
      "----------------------------------\n",
      "Predicting with Model_5\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = -0.00301\n",
      "- Prediction Min = -0.00415\n",
      "- Prediction Max = 0.00848\n",
      "Calculating HC Error\n",
      "- HC Error  = 3.59e+08\n",
      "----------------------------------\n",
      "============== MODEL 5 PROCESS END ==============\n",
      " \n",
      "Model_6 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.98\n",
      "- Scaler: Standard\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 9\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_6 Created\n",
      "----------------------------------\n",
      "Model_6 - Training\n",
      "- Started on 05-16 at 04:10\n",
      "Model_6 - Training Complete\n",
      "- Time: 42.062 min\n",
      "- Loss = 87.96344\n",
      "- Val Loss = 88.95634\n",
      "----------------------------------\n",
      "Predicting with Model_6\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04100\n",
      "- Prediction Min = 0.00147\n",
      "- Prediction Max = 0.05132\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.22e+10\n",
      "----------------------------------\n",
      "============== MODEL 6 PROCESS END ==============\n",
      " \n",
      "Model_7 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.95\n",
      "- Scaler: MinMax\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 6\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_7 Created\n",
      "----------------------------------\n",
      "Model_7 - Training\n",
      "- Started on 05-16 at 04:52\n",
      "Model_7 - Training Complete\n",
      "- Time: 42.308 min\n",
      "- Loss = 802.59167\n",
      "- Val Loss = 107.07012\n",
      "----------------------------------\n",
      "Predicting with Model_7\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = -0.00000\n",
      "- Prediction Min = -0.00099\n",
      "- Prediction Max = 0.00902\n",
      "Calculating HC Error\n",
      "- HC Error  = 9.25e+07\n",
      "----------------------------------\n",
      "============== MODEL 7 PROCESS END ==============\n",
      " \n",
      "Model_8 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.95\n",
      "- Scaler: Standard\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 8\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_8 Created\n",
      "----------------------------------\n",
      "Model_8 - Training\n",
      "- Started on 05-16 at 05:35\n",
      "Model_8 - Training Complete\n",
      "- Time: 42.669 min\n",
      "- Loss = 88.56917\n",
      "- Val Loss = 88.80067\n",
      "----------------------------------\n",
      "Predicting with Model_8\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04291\n",
      "- Prediction Min = -0.01697\n",
      "- Prediction Max = 0.05261\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.65e+10\n",
      "----------------------------------\n",
      "============== MODEL 8 PROCESS END ==============\n",
      " \n",
      "Model_9 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.9\n",
      "- Scaler: MinMax\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 5\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_9 Created\n",
      "----------------------------------\n",
      "Model_9 - Training\n",
      "- Started on 05-16 at 06:18\n",
      "Model_9 - Training Complete\n",
      "- Time: 42.910 min\n",
      "- Loss = 828.50031\n",
      "- Val Loss = 111.68316\n",
      "----------------------------------\n",
      "Predicting with Model_9\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.00261\n",
      "- Prediction Min = 0.00180\n",
      "- Prediction Max = 0.01680\n",
      "Calculating HC Error\n",
      "- HC Error  = 3.63e+08\n",
      "----------------------------------\n",
      "============== MODEL 9 PROCESS END ==============\n",
      " \n",
      "Model_10 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.9\n",
      "- Scaler: Standard\n",
      "----------------------------------\n",
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 7\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_10 Created\n",
      "----------------------------------\n",
      "Model_10 - Training\n",
      "- Started on 05-16 at 07:00\n",
      "Model_10 - Training Complete\n",
      "- Time: 43.209 min\n",
      "- Loss = 90.64018\n",
      "- Val Loss = 88.91298\n",
      "----------------------------------\n",
      "Predicting with Model_10\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04391\n",
      "- Prediction Min = 0.01105\n",
      "- Prediction Max = 0.05154\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.75e+10\n",
      "----------------------------------\n",
      "============== MODEL 10 PROCESS END ==============\n",
      " \n",
      "Creating DataFrame\n",
      "Changing DataFrame column names\n",
      "Ranking Models\n"
     ]
    }
   ],
   "source": [
    "HC_ranking, models, histories = process_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Variance Explained</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>MSPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model_7</td>\n",
       "      <td>0.95</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>9.253633e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model_3</td>\n",
       "      <td>0.99</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>2.016400e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model_5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>3.592826e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Model_9</td>\n",
       "      <td>0.90</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>3.633233e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>MinMax</td>\n",
       "      <td>6.253711e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model_2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Standard</td>\n",
       "      <td>6.062462e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model_4</td>\n",
       "      <td>0.99</td>\n",
       "      <td>Standard</td>\n",
       "      <td>6.080919e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model_6</td>\n",
       "      <td>0.98</td>\n",
       "      <td>Standard</td>\n",
       "      <td>6.216547e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model_8</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Standard</td>\n",
       "      <td>6.652785e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model_10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Standard</td>\n",
       "      <td>6.745921e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Variance Explained    Scaler          MSPE\n",
       "6   Model_7                0.95    MinMax  9.253633e+07\n",
       "2   Model_3                0.99    MinMax  2.016400e+08\n",
       "4   Model_5                0.98    MinMax  3.592826e+08\n",
       "8   Model_9                0.90    MinMax  3.633233e+08\n",
       "0   Model_1                1.00    MinMax  6.253711e+08\n",
       "1   Model_2                1.00  Standard  6.062462e+10\n",
       "3   Model_4                0.99  Standard  6.080919e+10\n",
       "5   Model_6                0.98  Standard  6.216547e+10\n",
       "7   Model_8                0.95  Standard  6.652785e+10\n",
       "9  Model_10                0.90  Standard  6.745921e+10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HC_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_6 Standard\n"
     ]
    }
   ],
   "source": [
    "number = 5\n",
    "print(models[number].name + ' ' + HC_ranking['Scaler'][number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vector=np.linspace(1,epochs,epochs)\n",
    "\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    history = histories[i]\n",
    "    scaler_name = HC_ranking['Scaler'][i]\n",
    "    var_exp = HC_ranking['Variance Explained'][i]\n",
    "    \n",
    "    model.save(os.path.join(output_path,'{}-Var_{}-{}.h5'.format(model.name, var_exp, scaler_name)))\n",
    "    \n",
    "    hist_data =[epoch_vector,history.history['loss'],history.history['val_loss']]\n",
    "    hist_data =pd.DataFrame(hist_data).transpose()\n",
    "    hist_data.columns=['Epochs','loss','val_loss']\n",
    "    \n",
    "    hist_data.to_csv(os.path.join(output_path,'Training_History_{}.csv'.format(model.name)),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models and the training histories will be moved to foldr **Gen 7**. \n",
    "\n",
    "Nothing in this notebook yielded an improvement in performance. \n",
    "\n",
    "Maybe it has to do with the setup of the problem, or the setup of the database, or simply there is no relationship between the chosen inputs and the chosen outputs. \n",
    "\n",
    "A possible source of error is the cheating done by automakers on these standarized EPA tests from which all the data is derived. It **could** be possible that there **is** a relationship between inputs and outputs, but because many of the outputs are fake, this relationship is destroyed and the neural networks can't learn how to represent it. \n",
    "\n",
    "The **FINAL** attempt to predict something will drastically reduce the data sets. Different data-sets will be created where there is only **ONE** car manufacturer (maybe the top 2 or 3 manufacturers with the most cars will be chosen), and the same procedure will be applied to see if in that limited application, neural networks are capable of finding something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
