{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In the past notebook it was found that some input variables were pushing the predictions to negative values. Having identified them, one more attempt will be made to predict emissions by implementing some new techniques that have come to light by reading even more literature. \n",
    "\n",
    "1) Maybe using the **MinMaxScaler** wasn't the best option. Using the **Standard Scaler** yields data with mean of **0** and variance of **1**. Will this have an impact in the algorithm?\n",
    "\n",
    "2) Having applied a standard scaler to the data, PCA (Principal Component Analysis) will be implemented to see if this algorithm can identify the most important input variables.\n",
    "\n",
    "    1) It's important to take into account the results of the past notebook. The variables that pushed the predictions to be negative were:\n",
    "    \n",
    "        1) Fuel Code\n",
    "        2) Drive System\n",
    "        3) Peak_pos (Positive acceleration peaks)\n",
    "        4) ETW (Estimated Test Weight)\n",
    "        5) a_pos (Positive acceleration mean)\n",
    "        6) HP (Horse Power)\n",
    "\n",
    "The models that will be used during this notebook will be: \n",
    "\n",
    "* **MAPE + Leaky ReLU + 20% dropout + linear output**\n",
    "* **MAPE + Leaky ReLU + 10% dropout + linear output**\n",
    "\n",
    "This is in hopes of trying to keep everything as close as possible between notebooks and find the error in the prediction process. After this notebook has been implemented, another smaller hyper-parameter search can be done to see if predictions can be improved. \n",
    "\n",
    "For the sake of continuity, only **HC** will be predicted in this run. If something promising is found, other pollutants will be attempted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO's\n",
    "\n",
    "Data Scaling\n",
    "* Import scaled data\n",
    "* Import scalers\n",
    "* Inverse scale data\n",
    "* Create new scalers using **Standard Scaler**\n",
    "* Scale the data \n",
    "\n",
    "Data Sets\n",
    "* Create the data sets the same way as before (without shuffling to keep the same order)\n",
    "\n",
    "PCA\n",
    "* Apply different instances of PCA keeping a different amount of variables.\n",
    "    * The PCA function receives one input: n_componentes\n",
    "        * If **0 < n_components < 1**, this number represents the minimum amount of variance that needs to be explained by the selected components\n",
    "        * If **1 $\\leq$ n_components $\\leq$ number of input parameters**, this number represents the number of components to be kept\n",
    "* Explain the following amount of variance:\n",
    "    * 99%\n",
    "    * 95%\n",
    "    * 90%\n",
    "    * 80%\n",
    "* Create a new training set according to the PCA instances\n",
    "\n",
    "Machine Learning\n",
    "* Create models for each training set\n",
    "* Train\n",
    "* Predict\n",
    "* Calculate error\n",
    "* Rank models\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drllc\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Input, Dense, Dropout, advanced_activations, BatchNormalization, LeakyReLU\n",
    "from keras import losses, optimizers, activations\n",
    "import keras.backend as K\n",
    "\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join('.','output')\n",
    "minmax_scaler_path = os.path.join('.','Scalers','MinMax')\n",
    "standard_scaler_path = os.path.join('.','Scalers','Standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Original Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "complete_data_scaled_shuffled = pd.read_csv('Dataset_Scaled_Shuffled.csv')\n",
    "print('Shuffled dataset loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MinMax Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to put all the scalers\n",
    "minmax_scalers = []\n",
    "\n",
    "for i in range(len(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    scaler_filename = os.path.join(minmax_scaler_path,'scaler{}.save'.format(i))\n",
    "    minmax_scaler = joblib.load(scaler_filename)\n",
    "    \n",
    "    minmax_scalers.append(minmax_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with feature: Year\n",
      "Success with feature: Vehicle_Code\n",
      "Success with feature: Manufacturer_Code\n",
      "Success with feature: Displacement\n",
      "Success with feature: Fuel_System\n",
      "Success with feature: Gears\n",
      "Success with feature: Transmission_Code\n",
      "Success with feature: ETW\n",
      "Success with feature: HP\n",
      "Success with feature: Drive_System_Code\n",
      "Success with feature: Fuel_Code\n",
      "Success with feature: V_avg\n",
      "Success with feature: V_max\n",
      "Success with feature: V_std\n",
      "Success with feature: a_pos\n",
      "Success with feature: a_neg\n",
      "Success with feature: Peak_pos\n",
      "Success with feature: Peak_neg\n",
      "Success with feature: HC\n",
      "Success with feature: CO\n",
      "Success with feature: CO2\n",
      "Success with feature: Nox\n"
     ]
    }
   ],
   "source": [
    "# First, inverse transform all original values from the test_set\n",
    "original_data_inverse = complete_data_scaled_shuffled.copy()\n",
    "\n",
    "for i in range(len(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    col_name = complete_data_scaled_shuffled.columns[i]\n",
    "    \n",
    "    values = original_data_inverse[col_name].values\n",
    "    values = values.astype('float64')\n",
    "    values = values.reshape(values.shape[0],1)\n",
    "    \n",
    "    original_data_inverse[col_name] = minmax_scalers[i].inverse_transform(values)\n",
    "    \n",
    "    print('Success with feature: {}'.format(col_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with feature: Year\n",
      "Success with feature: Vehicle_Code\n",
      "Success with feature: Manufacturer_Code\n",
      "Success with feature: Displacement\n",
      "Success with feature: Fuel_System\n",
      "Success with feature: Gears\n",
      "Success with feature: Transmission_Code\n",
      "Success with feature: ETW\n",
      "Success with feature: HP\n",
      "Success with feature: Drive_System_Code\n",
      "Success with feature: Fuel_Code\n",
      "Success with feature: V_avg\n",
      "Success with feature: V_max\n",
      "Success with feature: V_std\n",
      "Success with feature: a_pos\n",
      "Success with feature: a_neg\n",
      "Success with feature: Peak_pos\n",
      "Success with feature: Peak_neg\n",
      "Success with feature: HC\n",
      "Success with feature: CO\n",
      "Success with feature: CO2\n",
      "Success with feature: Nox\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to put all the scalers\n",
    "standard_scalers = []\n",
    "\n",
    "for i in range(len(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    standard_scalers.append(standard_scaler)\n",
    "    \n",
    "# Copy the data set to avoid altering the original\n",
    "new_data_scaled = original_data_inverse.copy()\n",
    "    \n",
    "# Loop over the standard_scalers and perform the scaling operation on each column\n",
    "for i in range(len(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    col_name = complete_data_scaled_shuffled.columns[i]\n",
    "    \n",
    "    values = new_data_scaled[col_name].values\n",
    "    values = values.astype('float64')\n",
    "    values = values.reshape(values.shape[0],1)\n",
    "    \n",
    "    new_data_scaled[col_name] = standard_scalers[i].fit_transform(values)\n",
    "    \n",
    "    print('Success with feature: {}'.format(col_name))\n",
    "    \n",
    "# Export scalers for later use\n",
    "for i in range(len(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    scaler_filename = os.path.join(standard_scaler_path,'scaler{}.save'.format(i))\n",
    "    standard_scaler = standard_scalers[i]\n",
    "    joblib.dump(standard_scaler, scaler_filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size = 62511\n",
      "Dev Size = 7814\n",
      "Test Size = 7814\n",
      "Remainder = 0\n"
     ]
    }
   ],
   "source": [
    "# Get number of data points\n",
    "data_points = complete_data_scaled_shuffled.shape[0]\n",
    "\n",
    "# Set sizes for train, dev, test sets\n",
    "train_percent = 0.8\n",
    "train_size = round(train_percent*data_points)\n",
    "\n",
    "if (data_points-train_size)%2 == 0:\n",
    "    dev_size = int((data_points-train_size)/2)\n",
    "    test_size = dev_size\n",
    "    print('Train Size = {}'.format(train_size))\n",
    "    print('Dev Size = {}'.format(dev_size))\n",
    "    print('Test Size = {}'.format(test_size))\n",
    "    print('Remainder = {}'.format(train_size+dev_size+test_size-data_points))\n",
    "    \n",
    "else:\n",
    "    train_size = train_size-1\n",
    "    dev_size = int((data_points-train_size)/2)\n",
    "    test_size = dev_size \n",
    "    print('Train Size = {}'.format(train_size))\n",
    "    print('Dev Size = {}'.format(dev_size))\n",
    "    print('Test Size = {}'.format(test_size))\n",
    "    print('Remainder = {}'.format(train_size+dev_size+test_size-data_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data-sets\n",
      "Splitting into inputs and outputs\n",
      "Data-sets complete\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a local copy of the entire NEW dataset\n",
    "data_scaled_shuffled = new_data_scaled.copy()\n",
    "\n",
    "print('Preparing Data-sets')\n",
    "# Divide data into train, dev, and test sets\n",
    "train_set = data_scaled_shuffled[ : train_size]\n",
    "dev_set = data_scaled_shuffled[train_size : train_size+dev_size]\n",
    "test_set = data_scaled_shuffled[train_size+dev_size : train_size+dev_size+test_size]\n",
    "\n",
    "# Reset index for all sets\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "dev_set = dev_set.reset_index(drop=True)\n",
    "test_set = test_set.reset_index(drop=True)\n",
    "\n",
    "# Get values\n",
    "train_set_values = train_set.values\n",
    "dev_set_values = dev_set.values\n",
    "test_set_values = test_set.values\n",
    "\n",
    "# Number of emissions: HC, CO, CO2, NOX\n",
    "n_out = 4\n",
    "\n",
    "print('Splitting into inputs and outputs')\n",
    "# SLICING: [start row:end row , start column:end column]\n",
    "# Split into inputs and outputs\n",
    "original_x_train = train_set_values[:,:-n_out]\n",
    "original_x_dev = dev_set_values[:,:-n_out]\n",
    "original_x_test = test_set_values[:,:-n_out]\n",
    "\n",
    "# Get the outputs (only HC)\n",
    "HC_train = train_set_values[:,-n_out]\n",
    "HC_dev = dev_set_values[:,-n_out]\n",
    "HC_test = test_set_values[:,-n_out]\n",
    "\n",
    "print('Data-sets complete')\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with feature: Year\n",
      "Success with feature: Vehicle_Code\n",
      "Success with feature: Manufacturer_Code\n",
      "Success with feature: Displacement\n",
      "Success with feature: Fuel_System\n",
      "Success with feature: Gears\n",
      "Success with feature: Transmission_Code\n",
      "Success with feature: ETW\n",
      "Success with feature: HP\n",
      "Success with feature: Drive_System_Code\n",
      "Success with feature: Fuel_Code\n",
      "Success with feature: V_avg\n",
      "Success with feature: V_max\n",
      "Success with feature: V_std\n",
      "Success with feature: a_pos\n",
      "Success with feature: a_neg\n",
      "Success with feature: Peak_pos\n",
      "Success with feature: Peak_neg\n",
      "Success with feature: HC\n",
      "Success with feature: CO\n",
      "Success with feature: CO2\n",
      "Success with feature: Nox\n"
     ]
    }
   ],
   "source": [
    "# Inverse transform the TEST DATA to be able to calculate the error further down\n",
    "test_set_scaled = complete_data_scaled_shuffled[train_size+dev_size : train_size+dev_size+test_size]\n",
    "test_set_inverse = test_set_scaled.copy()\n",
    "\n",
    "for i in range(np.size(complete_data_scaled_shuffled.columns)):\n",
    "    \n",
    "    col_name = complete_data_scaled_shuffled.columns[i]\n",
    "    \n",
    "    values = test_set_inverse[col_name].values\n",
    "    values = values.astype('float64')\n",
    "    values = values.reshape(values.shape[0],1)\n",
    "    \n",
    "    test_set_inverse[col_name] = minmax_scalers[i].inverse_transform(values)\n",
    "    \n",
    "    print('Success with feature: {}'.format(col_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Create a function that creates a PCA instance and with that PCA instance creates a new x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca_set(variance_amount):\n",
    "    \n",
    "    print('Create PCA Instance')\n",
    "    pca = PCA(variance_amount)\n",
    "    \n",
    "    print('Fit PCA Instance')\n",
    "    pca.fit(original_x_train)\n",
    "    print('Number of Components = {}'.format(pca.n_components_))\n",
    "    \n",
    "    print('Create New Input Training Set')\n",
    "    new_x_train = pca.transform(original_x_train)\n",
    "    \n",
    "    print('Create New Input Dev Set')\n",
    "    new_x_dev = pca.transform(original_x_dev)\n",
    "    \n",
    "    print('Create New Input Test Set')\n",
    "    new_x_test = pca.transform(original_x_test)\n",
    "    \n",
    "    print('----------------------------------')\n",
    "    \n",
    "    return new_x_train, new_x_dev, new_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_options = [0.99,0.95,0.90,0.80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch size, epochs\n",
    "batch_size = 64\n",
    "epochs = 300\n",
    "dropouts = [0.1, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with MAPE, Leaky ReLU, Adam\n",
    "def build_model(number, x_train, dd):\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential(name='Model_{}'.format(number))\n",
    "\n",
    "    model.add(Dense(256, input_dim=x_train.shape[1]))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(advanced_activations.LeakyReLU())\n",
    "    model.add(Dropout(dd))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    #Compile model\n",
    "    model.compile(loss=losses.mean_absolute_percentage_error, optimizer=optimizers.Adam(), metrics = ['accuracy'])\n",
    "    \n",
    "    print('{} Created'.format(model.name))\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model, x_train, y_train, x_dev, y_dev):\n",
    "    \n",
    "    print('{} - Training'.format(model.name))\n",
    "    print('- Started on {} at {}'.format(str(datetime.datetime.now())[5:-16], str(datetime.datetime.now())[11:-10]))\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                        validation_data=(x_dev, y_dev), verbose=0, shuffle=True)\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time() - start_time\n",
    "    print('{} - Training Complete'.format(model.name))\n",
    "    print('- Time: {:.3f} min'.format(end_time/60))\n",
    "    print('- Loss = {:.5f}'.format(history.history['loss'][-1]))\n",
    "    print('- Val Loss = {:.5f}'.format(history.history['val_loss'][-1]))\n",
    "    print('----------------------------------')\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions and Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define MSPE\n",
    "def msp_error(true,pred):\n",
    "    error = 100*np.sum(((true-pred)/true)**2)/np.size(true)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_get_error(model, x_test):\n",
    "    \n",
    "    print('Predicting with {}'.format(model.name))\n",
    "    scaled_predictions = model.predict(x_test)\n",
    "    \n",
    "    print('Inverse Scaling Operation') \n",
    "     \n",
    "    # Inverse the scaling operation on the predictions\n",
    "    predictions = standard_scalers[-4].inverse_transform(scaled_predictions)\n",
    "    \n",
    "    print('- Prediction Mean = {:.5f}'.format(np.mean(predictions)))\n",
    "    print('- Prediction Min = {:.5f}'.format(np.min(predictions)))\n",
    "    print('- Prediction Max = {:.5f}'.format(np.max(predictions)))\n",
    "\n",
    "    print('Calculating HC Error')\n",
    "    mspe = msp_error(test_set_inverse['HC'].values, predictions)\n",
    "        \n",
    "    print('- HC Error  = {:.2e}'.format(mspe))\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    return mspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Models and Rank with MSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_models():\n",
    "    \n",
    "    count = 1\n",
    "    model_list = []\n",
    "    history_list = []\n",
    "    HC_error_list = []\n",
    "\n",
    "    for variance_percent in pca_options:\n",
    "        \n",
    "        for dd in dropouts:\n",
    "\n",
    "            # Print model variables\n",
    "            print('Model_{} Variables:'.format(count))\n",
    "            print('- Loss: MAPE')\n",
    "            print('- Activation: Leaky ReLU')\n",
    "            print('- Optimizer: Adam')\n",
    "            print('- Dropout: {}%'.format(dd*100))\n",
    "            print('- Variance Explained: {}'.format(variance_percent))\n",
    "            print('----------------------------------')\n",
    "\n",
    "            # Get the INPUT datasets after PCA\n",
    "            x_train, x_dev, x_test = create_pca_set(variance_percent)\n",
    "\n",
    "            # Get OUTPUT datasets\n",
    "            y_train = HC_train\n",
    "            y_dev = HC_dev\n",
    "            y_test = HC_test\n",
    "\n",
    "            # Create model\n",
    "            model = build_model(count, x_train, dd)\n",
    "\n",
    "            # Train model\n",
    "            history = train_models(model, x_train, y_train, x_dev, y_dev)\n",
    "            history_list.append(history)\n",
    "\n",
    "            # Make predictions and calculate error\n",
    "            error = predict_get_error(model, x_test)\n",
    "\n",
    "            # Add error to error list\n",
    "            HC_error_list.append([model.name, variance_percent, error])\n",
    "\n",
    "            # Announce one model process ended\n",
    "            print('============== MODEL {} PROCESS END =============='.format(count))\n",
    "            print(' ')\n",
    "\n",
    "            # Increase counter by 1\n",
    "            count = count+1\n",
    "\n",
    "            # Add TRAINED model to list\n",
    "            model_list.append(model)\n",
    "\n",
    "    print('Creating DataFrame')                \n",
    "    HC_error = pd.DataFrame(HC_error_list)\n",
    "\n",
    "    print('Changing DataFrame column names')\n",
    "    HC_error.columns = ['Model', 'Variance Explained', 'MSPE']\n",
    "\n",
    "    print('Ranking Models')\n",
    "    HC_error.sort_values(by=['MSPE'], inplace=True)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    return HC_error, model_list, history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 10.0%\n",
      "- Variance Explained: 0.99\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 12\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_1 Created\n",
      "----------------------------------\n",
      "Model_1 - Training\n",
      "- Started on 04-02 at 17:23\n",
      "Model_1 - Training Complete\n",
      "- Time: 41.832 min\n",
      "- Loss = 83.95899\n",
      "- Val Loss = 88.66776\n",
      "----------------------------------\n",
      "Predicting with Model_1\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04011\n",
      "- Prediction Min = -0.05192\n",
      "- Prediction Max = 0.05140\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.05e+10\n",
      "----------------------------------\n",
      "============== MODEL 1 PROCESS END ==============\n",
      " \n",
      "Model_2 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.99\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 12\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_2 Created\n",
      "----------------------------------\n",
      "Model_2 - Training\n",
      "- Started on 04-02 at 18:04\n",
      "Model_2 - Training Complete\n",
      "- Time: 42.213 min\n",
      "- Loss = 86.21655\n",
      "- Val Loss = 85.76644\n",
      "----------------------------------\n",
      "Predicting with Model_2\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.03995\n",
      "- Prediction Min = -0.07295\n",
      "- Prediction Max = 0.05172\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.09e+10\n",
      "----------------------------------\n",
      "============== MODEL 2 PROCESS END ==============\n",
      " \n",
      "Model_3 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 10.0%\n",
      "- Variance Explained: 0.95\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 9\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_3 Created\n",
      "----------------------------------\n",
      "Model_3 - Training\n",
      "- Started on 04-02 at 18:47\n",
      "Model_3 - Training Complete\n",
      "- Time: 42.751 min\n",
      "- Loss = 85.49049\n",
      "- Val Loss = 88.77193\n",
      "----------------------------------\n",
      "Predicting with Model_3\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.03981\n",
      "- Prediction Min = -0.00193\n",
      "- Prediction Max = 0.05412\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.05e+10\n",
      "----------------------------------\n",
      "============== MODEL 3 PROCESS END ==============\n",
      " \n",
      "Model_4 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.95\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 9\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_4 Created\n",
      "----------------------------------\n",
      "Model_4 - Training\n",
      "- Started on 04-02 at 19:29\n",
      "Model_4 - Training Complete\n",
      "- Time: 42.566 min\n",
      "- Loss = 87.82995\n",
      "- Val Loss = 90.21129\n",
      "----------------------------------\n",
      "Predicting with Model_4\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04325\n",
      "- Prediction Min = -0.00271\n",
      "- Prediction Max = 0.05166\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.63e+10\n",
      "----------------------------------\n",
      "============== MODEL 4 PROCESS END ==============\n",
      " \n",
      "Model_5 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 10.0%\n",
      "- Variance Explained: 0.9\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 8\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_5 Created\n",
      "----------------------------------\n",
      "Model_5 - Training\n",
      "- Started on 04-02 at 20:12\n",
      "Model_5 - Training Complete\n",
      "- Time: 43.328 min\n",
      "- Loss = 86.74525\n",
      "- Val Loss = 91.99930\n",
      "----------------------------------\n",
      "Predicting with Model_5\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04062\n",
      "- Prediction Min = -0.00806\n",
      "- Prediction Max = 0.05139\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.21e+10\n",
      "----------------------------------\n",
      "============== MODEL 5 PROCESS END ==============\n",
      " \n",
      "Model_6 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.9\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 8\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_6 Created\n",
      "----------------------------------\n",
      "Model_6 - Training\n",
      "- Started on 04-02 at 20:55\n",
      "Model_6 - Training Complete\n",
      "- Time: 43.886 min\n",
      "- Loss = 87.97150\n",
      "- Val Loss = 89.04630\n",
      "----------------------------------\n",
      "Predicting with Model_6\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04043\n",
      "- Prediction Min = -0.00034\n",
      "- Prediction Max = 0.05140\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.16e+10\n",
      "----------------------------------\n",
      "============== MODEL 6 PROCESS END ==============\n",
      " \n",
      "Model_7 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 10.0%\n",
      "- Variance Explained: 0.8\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 6\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_7 Created\n",
      "----------------------------------\n",
      "Model_7 - Training\n",
      "- Started on 04-02 at 21:39\n",
      "Model_7 - Training Complete\n",
      "- Time: 43.908 min\n",
      "- Loss = 88.00016\n",
      "- Val Loss = 88.87170\n",
      "----------------------------------\n",
      "Predicting with Model_7\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04237\n",
      "- Prediction Min = -0.00768\n",
      "- Prediction Max = 0.05138\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.48e+10\n",
      "----------------------------------\n",
      "============== MODEL 7 PROCESS END ==============\n",
      " \n",
      "Model_8 Variables:\n",
      "- Loss: MAPE\n",
      "- Activation: Leaky ReLU\n",
      "- Optimizer: Adam\n",
      "- Dropout: 20.0%\n",
      "- Variance Explained: 0.8\n",
      "----------------------------------\n",
      "Create PCA Instance\n",
      "Fit PCA Instance\n",
      "Number of Components = 6\n",
      "Create New Input Training Set\n",
      "Create New Input Dev Set\n",
      "Create New Input Test Set\n",
      "----------------------------------\n",
      "Model_8 Created\n",
      "----------------------------------\n",
      "Model_8 - Training\n",
      "- Started on 04-02 at 22:23\n",
      "Model_8 - Training Complete\n",
      "- Time: 44.231 min\n",
      "- Loss = 90.66047\n",
      "- Val Loss = 87.03629\n",
      "----------------------------------\n",
      "Predicting with Model_8\n",
      "Inverse Scaling Operation\n",
      "- Prediction Mean = 0.04148\n",
      "- Prediction Min = -0.04443\n",
      "- Prediction Max = 0.05155\n",
      "Calculating HC Error\n",
      "- HC Error  = 6.38e+10\n",
      "----------------------------------\n",
      "============== MODEL 8 PROCESS END ==============\n",
      " \n",
      "Creating DataFrame\n",
      "Changing DataFrame column names\n",
      "Ranking Models\n"
     ]
    }
   ],
   "source": [
    "HC_ranking, models, histories = process_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Variance Explained</th>\n",
       "      <th>MSPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model_3</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6.047931e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model_1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6.048997e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model_2</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6.091472e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model_6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.163129e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model_5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.206516e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Model_8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>6.383961e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model_7</td>\n",
       "      <td>0.80</td>\n",
       "      <td>6.476902e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model_4</td>\n",
       "      <td>0.95</td>\n",
       "      <td>6.633811e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model  Variance Explained          MSPE\n",
       "2  Model_3                0.95  6.047931e+10\n",
       "0  Model_1                0.99  6.048997e+10\n",
       "1  Model_2                0.99  6.091472e+10\n",
       "5  Model_6                0.90  6.163129e+10\n",
       "4  Model_5                0.90  6.206516e+10\n",
       "7  Model_8                0.80  6.383961e+10\n",
       "6  Model_7                0.80  6.476902e+10\n",
       "3  Model_4                0.95  6.633811e+10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HC_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_vector=np.linspace(1,epochs,epochs)\n",
    "\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    history = histories[i]\n",
    "    \n",
    "    model.save(os.path.join(output_path,'{}'.format(model.name)))\n",
    "    \n",
    "    hist_data =[epoch_vector,history.history['loss'],history.history['val_loss']]\n",
    "    hist_data =pd.DataFrame(hist_data).transpose()\n",
    "    hist_data.columns=['Epochs','loss','val_loss']\n",
    "    \n",
    "    hist_data.to_csv(os.path.join(output_path,'Training_History_{}.csv'.format(model.name)),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying PCA made the predictions mostly positive (with some negatives), but did NOT improve the error. \n",
    "\n",
    "Using 20% dropout generally made the difference between the training loss and the validation loss smaller. \n",
    "\n",
    "The **LAST** attempt to get something will be:\n",
    "* Remove **all** the variables that made the predictions negative (found in the previous notebook)\n",
    "* Apply MinMax and Standard Scalers to compare the performance\n",
    "* Apply PCA to that data-set\n",
    "* Train models for the PCA-based data-sets and **one** model that uses the original inputs\n",
    "    * Make predictions\n",
    "    * Calculate error \n",
    "    * Rank models\n",
    "    * See if anything makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
